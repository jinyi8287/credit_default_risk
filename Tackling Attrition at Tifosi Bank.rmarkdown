---
title: "Tackling Attrition at Tifosi Bank"
format: pdf
editor: visual
---

```{r,message=FALSE,warning=FALSE}
#package
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(here)
library(readr)
tidymodels_prefer()
```

```{r,message=FALSE}
data<-read_csv(here("Data","bank_churners.csv")) %>% 
  clean_names()

```

```{r}
data
```

```{r}
skim(data)
```






## Convert categorical variables to factors where necessary




```{r}
#sapply()：是 R 裡用來「逐一作用到每個元素」的函數
names(data)[sapply(data, is.character)]

unique(data$attrition_flag)
unique(data$gender)
unique(data$education_level)
unique(data$marital_status)
unique(data$income_category)
unique(data$card_category)


data <- data %>% 
  mutate(
    attrition_flag = as.factor(attrition_flag),
    gender = as.factor(gender),
    education_level = as.factor(education_level),
    marital_status = as.factor(marital_status),
    income_category = as.factor(income_category),
    card_category = as.factor(card_category)
  )


```





## eda

#### missing value



```{r}
library(naniar)

miss_var_summary(data)
colSums(is.na(data))

```




#### factor(merging or not)



```{r,warning=FALSE,message=FALSE}
library(ggplot2)


data %>% 
  ggplot(aes(x = education_level)) +
  geom_bar(fill = "skyblue") +
  coord_flip() +  # 翻轉座標，類別名不會擠在一起
  labs(title = "Education Level Distribution",
       x = "Education Level",
       y = "Count")



library(dplyr)
library(forcats)

# 先計算每個 level 的比例
level_summary <- data %>%
  count(education_level) %>%
  mutate(prop = n / sum(n))  #	算出每個類別佔全部樣本的比例 

# 決定閾值（比如5%以下算 rare）
threshold <- 0.05

# 標記 rare / common
data <- data %>%
  left_join(level_summary, by = "education_level") %>%
  mutate(
    small_level = if_else(prop < threshold, "Rare", "Common")
  ) #把 level_summary 的 n 和 prop 合併到 data，以 education_level 為鍵

data %>%
  ggplot(aes(x = fct_infreq(education_level), fill = small_level)) +
  geom_bar() +
  coord_flip() +
  labs(title = "Education Level Distribution Highlighting Rare Levels",
       x = "Education Level",
       y = "Count") +
  scale_fill_manual(values = c("Common" = "skyblue", "Rare" = "red")) +
  theme_minimal()

```

```{r}
library(ggplot2)
library(dplyr)

# 指定要畫的變數
factor_vars <- c("attrition_flag", "gender", "education_level", 
                 "marital_status", "income_category", "card_category")

# 用 for loop 自動畫每個
for (var in factor_vars) {
  
  data %>%
    ggplot(aes_string(x = var)) +
    geom_bar(fill = "skyblue") +
    coord_flip() +
    labs(
      title = paste("Distribution of", var),
      x = var,
      y = "Count"
    ) +
    theme_minimal() -> p
  
  print(p)
}

```





#### response variable 



```{r,warning=FALSE}
library(ggplot2)

ggplot(data, aes(x = attrition_flag)) +
  geom_bar(fill = "skyblue") +
  geom_text(stat = "count", aes(label = after_stat(count)), hjust = 0.2) + 
  coord_flip() +  #after_stat(count)
  labs(title = "Attrition Flag Distribution",
       x = "Customer Status",
       y = "Count") +
  theme_minimal()

  
```






#### relationships among the numerical variables



```{r,message=FALSE}
#library(GGally)

#data %>% 
  # select(where(is.numeric)) %>% 
  #GGally::ggpairs(upper = list(continuous = wrap("cor", size = 2)))

#Pearson correlation 測試線性強度
```

```{r}
library(dplyr)
library(tidyr)

# 設定你的數值變數資料（只選 numeric）
numeric_data <- data %>% select(where(is.numeric))

# 計算 correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# 把 correlation matrix 轉成長格式
cor_df <- as.data.frame(as.table(cor_matrix))

# 避免重複（只保留上三角）
cor_df <- cor_df %>%
  filter(as.character(Var1) < as.character(Var2)) 

# 篩選出高相關變數對
high_cor <- cor_df %>%
  filter(abs(Freq) > 0.7) %>%
  arrange(desc(abs(Freq)))

# 顯示結果
high_cor

```

```{r}
library(ggplot2)

data %>%
  ggplot(aes(x = education_level, fill = attrition_flag)) +
  geom_bar(position = "fill") +
  coord_flip() +
  labs(
    x = "Education Level",
    y = "Proportion",
    title = "Proportion of Attrition by Education Level"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("skyblue", "red")) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))


```

```{r}
library(ggplot2)
library(dplyr)
library(forcats)

# 先列出你要分析的 categorical predictors
factor_vars <- c("gender", "education_level", "marital_status", "income_category", "card_category")

# 重新整理資料：轉成長格式（方便畫 facet_wrap）
data_long <- data %>%
  select(all_of(factor_vars), attrition_flag) %>%
  pivot_longer(cols = -attrition_flag, names_to = "predictor", values_to = "level")

# 畫圖
ggplot(data_long, aes(x = level, fill = attrition_flag)) +
  geom_bar(position = "fill") +
  facet_wrap(~ predictor, scales = "free_x") +
  coord_flip() +
  labs(
    title = "Proportion of Attrition by Categorical Predictors",
    x = "Levels",
    y = "Proportion"
  ) +
  scale_fill_manual(values = c("skyblue", "red")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))

```



#### promising predictor



```{r}
# 做交叉表
tab <- table(data$education_level, data$attrition_flag)

# 做卡方檢定
stats::chisq.test(tab)

```

```{r}
library(dplyr)
library(purrr)

# 你的 categorical predictors
factor_vars <- c("gender", "education_level", "marital_status", "income_category", "card_category")

# 自動跑卡方檢定
chi_sq_results <- map_dfr(factor_vars, function(var) {
  
  tab <- table(data[[var]], data$attrition_flag)
  
  test_result <- stats::chisq.test(tab)
  
  tibble(
    predictor = var,
    p_value = test_result$p.value,
    statistic = test_result$statistic
  )
})

# 查看結果
chi_sq_results %>%
  arrange(p_value)

```


- 1
p-value < 0.05	拒絕虛無假設（有顯著關聯），這個 predictor 和 attrition_flag 有統計上顯著關係，是 promising predictor

p-value ≥ 0.05	無法拒絕虛無假設（沒顯著關聯），這個 predictor 和 attrition_flag 沒有明顯關係，不是重點 predictor

- 2
那「類別 vs 類別」關聯該用什麼統計檢定？

情境	適合用的檢定方法
數值 vs 數值	Pearson correlation (r)
數值 vs 類別	t-test, ANOVA
類別 vs 類別	卡方檢定 (Chi-squared test)


-3

Response類型	Predictor類型	方法
類別 (2類)	類別	卡方檢定（Chi-squared test）
類別 (2類)	數值	t檢定（t-test）
類別 (>2類)	數值	ANOVA（變異數分析）





```{r}
library(dplyr)
library(purrr)
library(broom) # 讓 t.test 結果整理成表格

# 選出數值型變數
numeric_vars <- data %>% 
  select(where(is.numeric)) %>% 
  names()

# 自動對每個數值變數跑 t-test
t_test_results <- map_dfr(numeric_vars, function(var) {
  
  # 跑 t-test
  test_result <- t.test(data[[var]] ~ data$attrition_flag)
  
  # 整理成一行 tibble
  tidy_result <- broom::tidy(test_result)
  
  tibble(
    predictor = var,
    p_value = tidy_result$p.value,
    statistic = tidy_result$statistic
  )
})

# 查看結果
t_test_results %>%
  arrange(p_value)

```




# data split



```{r}
set.seed(47969938)
data_split<- initial_split(data, prop = 0.8, strata = attrition_flag)
train_data<- training(data_split)
test_data <- testing(data_split)
# Cross-validation folds
cross_validation_folds <- vfold_cv(train_data, v = 10, strata = attrition_flag)

```

```{r}
logistic_spec <- logistic_reg() |> 
    set_engine("glm") |> 
    set_mode("classification")

logistic_fit <- logistic_spec |> 
    fit(forested ~ . , data = forested_train)

pred_tb <- forested_test |> 
  select(forested) |> 
  bind_cols(predict(logistic_fit, forested_test, type = "prob")) |> 
  bind_cols(predict(logistic_fit, forested_test))

pred_tb
```

```{r}
# Define model
ridge_spec <-logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Define recipe---------------------------------------------------------------
reg_recipe <- recipe(log_expense ~ ., data = autoinsurance_train) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_normalize(all_numeric_predictors())
# Create workflow-------------------------------------------------------------
reg_wf <- workflow() |> 
  add_model(reg_model) |> 
  add_recipe(reg_recipe)
#Grid Search------------------------------------------------------------------
reg_param <- extract_parameter_set_dials(reg_model)
reg_grid <- grid_max_entropy(reg_param, size = 50)
# Tuning (Cross-validation)---------------------------------------------------
set.seed(47969938)
reg_tuned <- tune_grid(
  reg_wf,
  resamples = autoinsurance_folds,
  grid = reg_grid,
  metrics = metric_set(rmse, rsq)
)
#Select Best Parameters-------------------------------------------------------
reg_best <- select_best(reg_tuned, metric = "rmse")
final_reg_wf <- finalize_workflow(reg_wf, reg_best)
# Final model fitting---------------------------------------------------------
final_reg_fit <- final_reg_wf |> last_fit(autoinsurance_split)
# Evaluate on original scale(on test data)------------------------------------
final_reg_fit |> 
  collect_predictions() |> 
  mutate(
    .pred = exp(.pred),
    expenses = exp(log_expense)  # Back-transform for comparison
  ) |> 
  metrics(truth = expenses, estimate = .pred) |> 
  filter(.metric %in% c("rmse", "rsq"))

```

