---
title: "bank v3"
format: html
editor: visual
---

## Introduction
Credit card churn poses a critical threat to the profitability and long-term growth of retail banks. Credit card operations can contribute up to 25% of a bankâ€™s retail revenue, driven by interest income, interchange fees, annual charges, and cross-selling opportunities for other financial products and services (McKinsey, 2023). Therefore, a high churn rate not only implies a direct revenue loss but may also indicate declining customer satisfaction and loyalty.

For Tifosi Bank, a recent surge in credit card attrition highlights the urgency of understanding and addressing this issue. Identifying at-risk customers and uncovering the underlying drivers of churn can help inform targeted retention strategies and operational improvements.

This report applies predictive modeling techniques to analyze customer churn at Tifosi Bank. By evaluating both behavioral and demographic indicators, we aim to uncover key patterns associated with attrition and develop a robust classification model to support proactive customer retention efforts.





Credit card churn poses a significant threat to the profitability, customer lifetime value, and strategic growth of retail banks. With credit card operations contributing up to 25% of a bankâ€™s retail revenueâ€”driven by interest income, interchange fees, annual charges, and cross-selling opportunities (McKinsey, 2023)â€”a rising churn rate represents not just a revenue loss but a potential erosion of customer trust and competitive positioning.

For Tifosi Bank, a recent spike in credit card attrition raises concerns about declining engagement and highlights the need for timely intervention. Left unaddressed, this trend may increase acquisition costs, weaken cross-sell pipelines, and undermine long-term market share. Identifying high-risk segments and understanding the key behavioral or demographic drivers of churn is essential to enable targeted, cost-effective retention strategies.

This report applies predictive modeling techniques to develop a churn classification framework tailored to Tifosi Bankâ€™s credit card portfolio. By leveraging machine learning on behavioral and demographic data, we aim to provide actionable insights that inform retention planning, resource allocation, and customer experience managementâ€”ultimately helping leadership mitigate risk and sustain growth in a highly competitive market.

## Data Preprocessing

| Category              | Description                                                                 |
|-----------------------|-----------------------------------------------------------------------------|
| **Objective**         | Prepare features for modeling by cleaning and transforming variables.       |
| **Numerical Features**| - Removed ID column `clientnum` <br> - Applied standardization (`step_normalize`) |
| **Categorical Features**| - Converted all categorical predictors to dummy variables (`step_dummy`) <br> - Removed predictors with no variance (`step_zv`) |
| **Missing Values**    | - No missing values in the dataset <br> - No imputation needed |
| **Data Split**        | - Stratified 80/20 split by `attrition_flag` <br> - Ensured class balance |




ä½ åˆ†æçš„æ•´é«”æµç¨‹ï¼ˆä¾‹å¦‚ï¼šè³‡æ–™æ¸…ç†ã€æ¨¡å‹é¸æ“‡ã€å¦‚ä½•é©—è­‰ï¼‰

ä½ ç‚ºä»€éº¼é€™æ¨£åšï¼ˆä¾‹å¦‚ï¼šç‚ºäº†é æ¸¬æµå¤±ã€æ‰¾å‡ºä¸»è¦é©…å‹•å› å­ï¼‰

é€™æ¨£çš„æµç¨‹æ€éº¼å¹«åŠ©éŠ€è¡Œåšå‡ºæ±ºç­–ï¼ˆä¾‹å¦‚ï¼šç²¾æº–é–å®šé«˜é¢¨éšªå®¢æˆ¶ï¼‰



| è©•åˆ†è¦æ±‚                                  | å¯¦è¸æ–¹å¼                                                          |
| ------------------------------------- | ------------------------------------------------------------- |
| Exceptionally clear and succinct      | åˆªæ‰æ‰€æœ‰ã€ŒæŠ€è¡“è¡“èªã€å¦‚ `glmnet`ã€`tune_grid`ï¼Œæ”¹ç”¨ã€Œæˆ‘å€‘ä½¿ç”¨äº†ä¸€ç¨®ç¶“å¸¸æ‡‰ç”¨æ–¼å®¢æˆ¶æµå¤±é æ¸¬çš„å»ºæ¨¡æ–¹å¼â€¦ã€ |
| Effectively simplifies the process    | ç”¨ç™½è©±æè¿°ï¼Œä¾‹å¦‚ï¼šã€Œæˆ‘å€‘å…ˆè§€å¯Ÿäº†å“ªäº›å®¢æˆ¶æœ€å¸¸æµå¤±ï¼Œæ¥è‘—åˆ©ç”¨å¤šç¨®æ¨¡å‹æ¸¬è©¦é æ¸¬æ•ˆæœâ€¦ã€                     |
| Tailored for a non-technical audience | å‡è¨­ä½ çš„è®€è€…æ˜¯é«˜å±¤æˆ–å®¢æˆ¶ç¶“ç†ï¼Œèªªã€Œç‚ºäº†è®“è¡ŒéŠ·éƒ¨é–€èƒ½å¿«é€Ÿè¾¨è­˜é¢¨éšªå®¢æˆ¶ï¼Œæˆ‘å€‘å»ºç«‹äº†ä¸€å€‹ç°¡å–®ä¸”æº–ç¢ºçš„æ¨¡å‹â€¦ã€           |





# Analytical Overview


We began by examining the dataset to understand its structure and ensure data quality. This included identifying variable types, checking for missing values, and confirming that the engineered featuresâ€”such as transaction changes over timeâ€”were valid and meaningful. We then conducted exploratory analysis to examine how churn rates varied across customer subgroups, such as card types, age brackets, and income levels, providing an early sense of the patterns behind attrition.

Following this, we developed a series of predictive modelsâ€”tools that estimate the likelihood of a customer churning based on past patterns. These models learn from the behavior and characteristics of previous customers who left the bank and apply that knowledge to identify others who may be at risk. To capture different types of patterns in the data, we applied both linear models (e.g., logistic regression) and non-linear models (e.g., K-Nearest Neighbors and Random Forest).

Each modelâ€™s performance was assessed using a set of industry-standard metrics, including accuracy, ROC AUC, precision, recall, and F1 score. While accuracy shows the overall proportion of correct predictions, it can be misleading when most customers do not churn. Therefore, we prioritized recallâ€”our ability to correctly flag actual churnersâ€”since missing those customers could mean significant revenue loss. At the same time, we monitored precision to ensure we didnâ€™t over-alert and waste resources on customers unlikely to churn.

To balance both concerns, we used the F1 score as a key metric that summarizes how well the model manages this trade-off.

Based on the best-performing model, we identified the most influential factors driving churn. These insights directly inform business strategy and suggest targeted actions Tifosi Bank can take to reduce customer attrition and improve long-term loyalty.


We began by examining the dataset to understand the structure and ensure data quality, including identifying variable types and checking for any missing values. Exploratory analysis was conducted to investigate the distribution of churn across different customer groupsâ€”such as card types, age brackets, and income levelsâ€”to uncover early patterns in customer attrition.

eda

```{r}
library(DiagrammeR)

grViz("
digraph analysis_flow {
  graph [layout = dot, rankdir = LR]
  
  node [shape = box, style = filled, color = lightblue]

  A [label = 'Raw Data']
  B [label = 'Data Cleaning\n(Missing values,\nData types)']
  C [label = 'Exploratory Data Analysis\n(EDA)']
  D [label = 'Modeling\n(Logistic, KNN, RF, etc.)']
  E [label = 'Model Evaluation\n(ROC, Recall, F1)']
  F [label = 'Key Findings & Insights']
  G [label = 'Business Strategy\nRecommendations']

  A -> B -> C -> D -> E -> F -> G
}
")

```



```{r}
library(DiagrammeR)

grViz("
digraph tifosi_churn_flow {
  graph [layout = dot, rankdir = LR]
  node [shape = box, style = filled, color = lightblue, fontname = Helvetica]

  A [label = '1. Data Input\n(bank_churners.csv)']
  B [label = '2. Data Cleaning\n(Remove ID, Check NAs)']
  C [label = '3. EDA\n(Churn rate by group,\nBoxplots, Barcharts)']
  D [label = '4. Feature Engineering\n(Dummy, Normalize, Step_zv)']
  E [label = '5. Model Building\n(Logistic, KNN, RF, etc.)']
  F [label = '6. Cross-Validation\n(Grid Search, ROC, Recall)']
  G [label = '7. Model Evaluation\n(F1, Recall, Precision)']
  H [label = '8. Key Drivers\n(VIP, Interpretation)']
  I [label = '9. Business Actions\n(Retention Strategy, Targeting)']

  A -> B -> C -> D -> E -> F -> G -> H -> I
}
")
```






 (1) æ¸…æ¥šæ¨™å‡ºæ¯ä¸€é …ã€Œé—œéµç™¼ç¾ã€ï¼ˆKey Insightsï¼‰
å“ªäº›è®Šæ•¸ï¼ˆç‰¹å¾µï¼‰æ˜¯é æ¸¬æµå¤±çš„ä¸»å› ï¼Ÿï¼ˆä½¿ç”¨ VIP æ¢ç‹€åœ–ï¼‰

å“ªäº›ç¾¤é«”æµå¤±ç‡æœ€é«˜ï¼Ÿï¼ˆç”¨åˆ†çµ„æŸ±ç‹€åœ–æˆ– boxplotï¼‰

æ¨¡å‹å“ªå€‹æœ€å¥½ï¼Ÿç‚ºä»€éº¼ï¼Ÿï¼ˆæ”¾ä¸€å¼µæ€§èƒ½æŒ‡æ¨™æ¯”è¼ƒè¡¨æˆ–é›·é”åœ–ï¼‰

â¡ æ¯å€‹åœ–è¡¨æ—é‚Šéƒ½è¦å¯«ã€Œæ´å¯Ÿçµè«–ã€ä¸€å¥å…©å¥ï¼Œä¾‹å¦‚ï¼š

âœ… Customers with low Total_Trans_Ct and high Avg_Utilization_Ratio are significantly more likely to churn.

ğŸ“Œ (2) è®“åœ–è¡¨ç›´è§€ã€ç¾è§€ã€æ¸…æ¥š
ä¸è¦è¤‡é›œçš„ ggplot é è¨­ç°èƒŒæ™¯

åŠ å¤§å­—é«”ï¼ŒåŠ  title / caption

ä½¿ç”¨ color palette å€åˆ†é¡åˆ¥ï¼Œä¾‹å¦‚ churn vs non-churn

ğŸ“Š å»ºè­°åœ–è¡¨é¡å‹ï¼ˆé…åˆä½ ç›®å‰åˆ†æï¼‰ï¼š

VIP è®Šæ•¸é‡è¦æ€§åœ–

æ¨¡å‹æ¯”è¼ƒé›·é”åœ–ï¼ˆf_meas, recall ç‚ºè»¸ï¼‰

å®¢æˆ¶ç¾¤é«” vs æµå¤±ç‡çš„æŸ±ç‹€åœ–

Confusion matrix / ROC curveï¼ˆè‹¥ç°¡å–®æ˜ç­ï¼‰

ğŸ“Œ (3) æœ‰é‚è¼¯åœ°ä¸²é€£æ•…äº‹ï¼ˆStorytellingï¼‰
å¯ä¾ç…§é€™å€‹é‚è¼¯æ’åˆ—ï¼š

ğŸ” Who is churning?

ğŸ“ˆ What behaviors predict churn?

âš™ï¸ Which model performs best?

ğŸ¯ What actions can be taken?

é€™æ¨£æ¯æ®µåœ–è¡¨åˆ†æéƒ½è®Šæˆ æ•…äº‹çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ã€Œå­¤å³¶ã€ã€‚






```{r,message=FALSE,warning=FALSE}
#package
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(here)
library(readr)
tidymodels_prefer()
```



```{r,message=FALSE}
#load in data
raw_data<-read_csv(here("Data","bank_churners.csv")) %>% 
  clean_names()

```


```{r}
skim(raw_data)
raw_data

names(raw_data)
```



```{r}
# 3. æª¢æŸ¥ Attrition_Flag åˆ†å¸ƒ
raw_data %>%
  count(attrition_flag) %>%
  mutate(prop = n / sum(n))
```

#### variable description

| è®Šæ•¸åç¨±ï¼ˆè‹±æ–‡ï¼‰                   | ä¸­æ–‡è§£é‡‹            | èªªæ˜èˆ‡å»ºæ¨¡è§€å¯Ÿ                                         |
| -------------------------- | --------------- | ----------------------------------------------- |
| `clientnum`                | å®¢æˆ¶ç·¨è™Ÿ            | å”¯ä¸€è­˜åˆ¥ç¢¼ï¼Œå»ºæ¨¡æ™‚ä¸æœƒä½¿ç”¨ï¼ˆå”¯ä¸€ IDï¼‰                            |
| `attrition_flag`           | å®¢æˆ¶æ˜¯å¦æµå¤±          | ç›®æ¨™è®Šæ•¸ï¼š1 = æµå¤±ï¼Œ0 = ç•™å­˜                              |
| `customer_age`             | å®¢æˆ¶å¹´é½¡            | æ•¸å€¼è®Šæ•¸ï¼Œå¯èƒ½èˆ‡ä¿¡ç”¨ä½¿ç”¨è¡Œç‚ºæˆ–å¿ èª åº¦ç›¸é—œ                            |
| `gender`                   | æ€§åˆ¥              | é¡åˆ¥è®Šæ•¸ï¼Œ"M" æˆ– "F"ï¼Œå¯è¦–æƒ…æ³ä¿ç•™æˆ–è½‰æ›ç‚º dummy                 |
| `dependent_count`          | æ‰¶é¤Šäººæ•¸            | æ•´æ•¸ï¼Œå¯èƒ½èˆ‡æ”¯å‡ºèƒ½åŠ›èˆ‡å¿ èª åº¦ç›¸é—œ                                |
| `education_level`          | æ•™è‚²ç¨‹åº¦            | é¡åˆ¥è®Šæ•¸ï¼Œå¦‚ Graduateã€Post-Graduateã€High School ç­‰     |
| `marital_status`           | å©šå§»ç‹€æ…‹            | é¡åˆ¥è®Šæ•¸ï¼Œå¯èƒ½å½±éŸ¿è²¡å‹™è¡Œç‚ºèˆ‡ç©©å®šæ€§                               |
| `income_category`          | å¹´æ”¶å…¥ç¯„åœ           | é¡åˆ¥è®Šæ•¸ï¼ˆ\$40Kâ€“\$60Kã€\$60Kâ€“\$80K ç­‰ï¼‰ï¼Œå»ºè­°ä¿ç•™            |
| `card_category`            | ä¿¡ç”¨å¡é¡å‹           | é¡åˆ¥è®Šæ•¸ï¼ˆBlueã€Silverã€Goldã€Platinumï¼‰ï¼Œå¯æ¢ç´¢æ˜¯å¦èˆ‡ churn æœ‰é—œ |
| `months_on_book`           | é–‹æˆ¶æœˆä»½æ•¸           | å®¢æˆ¶èˆ‡éŠ€è¡Œé—œä¿‚é•·åº¦ï¼ˆå¿ èª åº¦ proxyï¼‰                            |
| `total_relationship_count` | ç”¢å“æ•¸é‡            | å®¢æˆ¶èˆ‡éŠ€è¡Œçš„é—œä¿‚æ·±åº¦ï¼ˆå¸³æˆ¶æ•¸ã€ç”¢å“æ•¸ç­‰ï¼‰                            |
| `months_inactive_12_mon`   | æœ€è¿‘ä¸€å¹´å…§çš„é–’ç½®æœˆä»½æ•¸     | å®¢æˆ¶é–’ç½®ç¨‹åº¦ï¼Œèˆ‡æµå¤±é«˜åº¦ç›¸é—œçš„è®Šæ•¸                               |
| `contacts_count_12_mon`    | æœ€è¿‘ä¸€å¹´å…§çš„è¯çµ¡æ¬¡æ•¸      | èˆ‡å®¢æœäº’å‹•æ¬¡æ•¸ï¼Œå¯åæ˜ æ»¿æ„åº¦æˆ–å•é¡Œ                               |
| `credit_limit`             | ä¿¡ç”¨å¡é¡åº¦           | å®¢æˆ¶æ ¸å®šçš„ä¿¡ç”¨é¡åº¦ï¼Œèˆ‡ä¿¡ç”¨è©•ç´šå’Œé¢¨éšªç®¡ç†ç›¸é—œ                          |
| `total_revolving_bal`      | å¾ªç’°ä¿¡ç”¨é¤˜é¡          | æœªå„Ÿé‚„é‡‘é¡ï¼Œå¯èƒ½ä»£è¡¨é¢¨éšªæˆ–æ´»èºåº¦                                |
| `avg_open_to_buy`          | å¹³å‡å¯ç”¨ä¿¡ç”¨é¡åº¦        | å¯ç”¨ä¿¡ç”¨ = ä¿¡ç”¨é¡åº¦ - é¤˜é¡ï¼Œä»£è¡¨å¯æ”¯é…ç©ºé–“                        |
| `total_amt_chng_q4_q1`     | ç¸½äº¤æ˜“é‡‘é¡è®Šå‹•ç‡ï¼ˆQ1â†’Q4ï¼‰ | è¡¨ç¤ºäº¤æ˜“é‡‘é¡çš„è®Šå‹•è¶¨å‹¢ï¼Œæ˜¯è¡Œç‚ºè®ŠåŒ–çš„æŒ‡æ¨™                            |
| `total_trans_amt`          | å¹´åº¦ç¸½äº¤æ˜“é‡‘é¡         | å®¢æˆ¶æ´»èºç¨‹åº¦ï¼ˆæ•´é«”ä½¿ç”¨æƒ…æ³ï¼‰                                  |
| `total_trans_ct`           | å¹´åº¦ç¸½äº¤æ˜“ç­†æ•¸         | å¦ä¸€å€‹æ´»èºæŒ‡æ¨™ï¼Œèˆ‡ä¸Šé¢æ­é…ä½¿ç”¨                                 |
| `total_ct_chng_q4_q1`      | ç¸½äº¤æ˜“ç­†æ•¸è®Šå‹•ç‡ï¼ˆQ1â†’Q4ï¼‰ | çœ‹äº¤æ˜“æ¬¡æ•¸æ˜¯å¦æ˜é¡¯ä¸‹é™æˆ–ä¸Šå‡                                  |
| `avg_utilization_ratio`    | ä¿¡ç”¨ä½¿ç”¨ç‡           | ä¿¡ç”¨å¡é¤˜é¡ä½”é¡åº¦æ¯”ç‡ï¼Œé«˜è€…å¯èƒ½ä»£è¡¨é¢¨éšªæˆ–é »ç¹ä½¿ç”¨è€…                       |





# eda




#### å“ªäº›æ—ç¾¤ churn rate é«˜ï¼Ÿ

###### churn rate in every income level

```{r}
# æŸ¥çœ‹é¡åˆ¥é †åº
unique(raw_data$income_category)

# å»ºç«‹äº¤å‰è¡¨ + ç™¾åˆ†æ¯”
income_tab <- raw_data %>%
  tabyl(income_category, attrition_flag) %>%
  adorn_percentages("row") %>%
  adorn_totals("row") %>%
  adorn_pct_formatting(digits = 1)

# é•·æ ¼å¼è³‡æ–™
income_long <- income_tab %>%
  filter(income_category != "Total") %>%
  pivot_longer(cols = -income_category, names_to = "attrition_flag", values_to = "percent")

# è¨ˆç®—å¯¦éš›ç­†æ•¸
income_counts <- raw_data %>%
  count(income_category, attrition_flag)

# åˆä½µè³‡æ–™
income_plot_data <- left_join(income_counts, income_long, by = c("income_category", "attrition_flag"))

# è¨­å®šé †åº
income_plot_data <- income_plot_data %>%
  mutate(income_category = fct_relevel(income_category,
    "< $40K", "$40K - $60K", "$60K - $80K", "$80K - $120K", "> $120K", "Unknown"
  ))

# ç¹ªåœ–
ggplot(income_plot_data, aes(x = income_category, y = n, fill = attrition_flag)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = percent),
            position = position_dodge(width = 0.9), vjust = -0.3, size = 4) +
  scale_fill_manual(values = c("#004c6d", "#a7c6ed")) +
  labs(
    title = "Customer Churn by Income Level",
    subtitle = "Based on 10,127 credit card customers",
    x = "Income Category",
    y = "Number of Customers"
  ) +
  theme_minimal() +
  theme(legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

```


```{r}
raw_data %>%
  count(income_category) %>%
  mutate(prop = n / sum(n)) %>%
  filter(income_category == "Unknown")

```


```{r}

income_churn <- raw_data %>%
  mutate(income_category = as.factor(income_category)) %>%  # å…ˆè½‰ factor
  group_by(income_category) %>%
  summarise(
    churn_rate = mean(attrition_flag == "Attrited Customer"),
    total = n()
  ) %>%
  mutate(income_category = fct_relevel(
    income_category,
    "Unknown", 
    "Less than $40K", 
    "$40K - $60K", 
    "$60K - $80K", 
    "$80K - $120K", 
    "$120K +"
  ))


ggplot(income_churn, aes(x = income_category, y = churn_rate)) +
  geom_col(fill = "#D72638") +
  geom_text(aes(label = scales::percent(churn_rate, accuracy = 0.1)), vjust = -0.3, size = 4) +
  labs(
    title = "Customer Churn Rate by Income Category",
    subtitle = "Proportion of customers who attrited in each income group",
    x = "Income Category",
    y = "Churn Rate (%)"
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )


```

###### churn rate in each education level


#### churn rate in each card category


```{r}
raw_data %>%
  count(card_category) %>%
  mutate(prop = n / sum(n)) 
```


```{r,message=FALSE,echo=FALSE}
unique(raw_data$card_category)

# äº¤å‰è¡¨ + ç™¾åˆ†æ¯”
card_tab <- raw_data %>%
  tabyl(card_category, attrition_flag) %>%
  adorn_percentages("row") %>%
  adorn_totals("row") %>%
  adorn_pct_formatting(digits = 1)

card_long <- card_tab %>%
  filter(card_category != "Total") %>%
  pivot_longer(cols = -card_category, names_to = "attrition_flag", values_to = "percent")


# è¨ˆç®—å¯¦éš›ç­†æ•¸
card_counts <- raw_data %>%
  count(card_category, attrition_flag)

#åˆä½µç‚ºç¹ªåœ–è³‡æ–™æ¡†
card_plot_data <- left_join(card_counts, card_long, by = c("card_category", "attrition_flag"))

#æ’åº
library(forcats)
card_plot_data <- card_plot_data %>%
  mutate(card_category = fct_relevel(card_category, "Blue", "Silver", "Gold", "Platinum"))


# ç¹ªåœ–
ggplot(card_plot_data, aes(x = card_category, y = n, fill = attrition_flag)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = percent),
            position = position_dodge(width = 0.9), vjust = -0.3, size = 4) +
  scale_fill_manual(values = c("#004c6d", "#a7c6ed")) +
  labs(
    title = "Customer Churn by Credit Card Category",
    subtitle = "Based on 10,127 credit card customers",
    x = "Card Category",
    y = "Number of Customers"
  ) +
  theme_minimal() +
  theme(legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

```




```{r}
# Step 1: è¨ˆç®—æ¯é¡å¡åˆ¥çš„æµå¤±ç‡èˆ‡ç¸½äººæ•¸
card_churn <- raw_data %>%
  mutate(card_category = as.factor(card_category)) %>%
  group_by(card_category) %>%
  summarise(
    churn_rate = mean(attrition_flag == "Attrited Customer"),
    total = n()
  ) %>%
  mutate(card_category = fct_relevel(
    card_category,
    "Blue", "Silver", "Gold", "Platinum"  # å¯èª¿æ•´é †åºï¼Œå¦‚è¦ Blue åœ¨å‰
  ))

# Step 2: ç¹ªåœ–
ggplot(card_churn, aes(x = card_category, y = churn_rate)) +
  geom_col(fill = "#5D3FD3") +
  geom_text(aes(label = scales::percent(churn_rate, accuracy = 0.1)), 
            vjust = -0.3, size = 4) +
  labs(
    title = "Customer Churn Rate by Card Category",
    subtitle = "Higher churn observed among Gold and Platinum cardholders",
    x = "Card Category",
    y = "Churn Rate (%)"
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )

```


äº¤æ˜“æ¬¡æ•°å¯¹æ¯”ï¼ˆåŸæ–‡ç¬¬9/26é¡µï¼‰

æ¨èå½¢å¼ï¼šèƒŒé èƒŒç®±çº¿å›¾ï¼ˆattrited vs existingå·¦å³å¯¹æ¯”ï¼‰

ä¼˜åŒ–å»ºè®®ï¼š

æ·»åŠ å‚è€ƒçº¿æ ‡æ³¨è¡Œä¸šå¹³å‡æ°´å¹³

æ ‡é¢˜æ”¹ä¸ºï¼š"æµå¤±å®¢æˆ·å¹´å‡äº¤æ˜“æ¬¡æ•°å‡å°‘XX%"

ä¿¡ç”¨é¢åº¦ä½¿ç”¨ç‡ï¼ˆåŸæ–‡ç¬¬10é¡µï¼‰

æ¨èå½¢å¼ï¼šåŒYè½´ç»„åˆå›¾ï¼ˆå·¦è¾¹ç®±çº¿å›¾+å³è¾¹æµå¤±ç‡æ›²çº¿ï¼‰

ä»·å€¼ç‚¹ï¼šå±•ç¤º"ä½¿ç”¨ç‡ä½äº20%çš„å®¢æˆ·æµå¤±é£é™©å¢åŠ XX%"

è´¦æˆ·ä¸æ´»è·ƒæœˆä»½æ•°ï¼ˆåŸæ–‡12-13/18é¡µï¼‰

æ¨èå½¢å¼ï¼šçƒ­åŠ›å›¾ï¼ˆæ¨ªè½´ï¼šä¸æ´»è·ƒæœˆä»½æ•°ï¼Œçºµè½´ï¼šæµå¤±ç‡ï¼‰

æ”¹è¿›å»ºè®®ï¼šæ ‡æ³¨å…³é”®é˜ˆå€¼"3ä¸ªæœˆä¸æ´»è·ƒæµå¤±é£é™©ç¿»å€"



## modeling

```{r,message=FALSE}
#package
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(here)
library(readr)
tidymodels_prefer()

raw_data<-read_csv(here("Data","bank_churners.csv")) %>% 
  clean_names() %>% 
  mutate(across(where(is.character), as_factor)) %>% 
  select(-clientnum)

```


# Data split

```{r}
set.seed(47969938)
data_split<- initial_split(raw_data, prop = 0.8, strata = attrition_flag)
train_data<- training(data_split)
test_data<- testing(data_split)
# Cross-validation folds
cross_validation_folds <- vfold_cv(train_data, v = 10, strata = attrition_flag)

```

## Logistic baseline

```{r}
# Define model
logistic_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Define recipe ---------------------------------------------------------------
logistic_recipe <- recipe(attrition_flag ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%            
  step_zv(all_predictors()) %>%                        
  step_normalize(all_numeric_predictors()) 


# Create workflow -------------------------------------------------------------
logistic_wf <- workflow() |> 
  add_model(logistic_spec) |> 
  add_recipe(logistic_recipe)

# Cross-validation: fit_resamples ---------------------------------------------
logistic_cv <- fit_resamples(
  logistic_wf,
  resamples = cross_validation_folds,
  metrics = metric_set(roc_auc, accuracy, precision, recall, f_meas),
  control = control_resamples(
    save_pred = TRUE,
    event_level = "second"
  )
)

# Evaluation on cross-validation folds ----------------------------------------
collect_metrics(logistic_cv)

# Last fit: ç”¨ train_data2 fitï¼Œpredict test_data2
final_logistic_fit <- last_fit(
  logistic_wf,
  split = data_split,
  metrics = metric_set( 
    roc_auc, 
    accuracy,
    recall,     
    precision,  
    f_meas      
))

# Collect performance on test set
collect_metrics(final_logistic_fit)

```

```{r}
logistic_preds <- collect_predictions(final_logistic_fit)
logistic_preds %>%
  conf_mat(truth = attrition_flag, estimate = .pred_class) %>%
  summary()

```



# Ridge Logistic Regression (mixture = 0) 


```{r}
# 1. Define model spec
ridge_spec <- logistic_reg(
  penalty = tune(),
  mixture = 0
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# 2. Create workflow
ridge_wf <- workflow() %>%
  add_model(ridge_spec) %>%
  add_recipe(logistic_recipe)

# 3. Define tuning grid
ridge_grid <- grid_regular(
  penalty(range = c(-4, 0)),
  levels = 30
)

# 4. Cross-validation tuning
ridge_tune <- tune_grid(
  ridge_wf,
  resamples = cross_validation_folds,  # ä¾‹å¦‚ vfold_cv(train_data, v = 10)
  grid = 50,
  metrics = metric_set(roc_auc, accuracy, precision, recall, f_meas),
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# 5. Select best model
best_ridge <- select_best(ridge_tune, metric = "roc_auc")

# 6. Finalize workflow
final_ridge_wf <- finalize_workflow(ridge_wf, best_ridge)

# 7. Last fit
final_ridge_fit <- last_fit(
  final_ridge_wf,
  split = data_split,
  metrics = metric_set(
    roc_auc, 
    accuracy,
    recall,
    precision,
    f_meas
  )
)

# 8. Collect test performance
collect_metrics(final_ridge_fit)


```






# Lasso Logistic Regression (mixture = 1) 


```{r}
# 1. Define model
lasso_spec <- logistic_reg(
  penalty = tune(),
  mixture = 1  # Lasso
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# 2. Create workflow
lasso_wf <- workflow() %>%
  add_model(lasso_spec) %>%
  add_recipe(logistic_recipe)  # ä¿ç•™ä½ åŸæœ¬çš„ä¹¾æ·¨ recipe

# 3. Cross-validation tuning (grid = 50 random)
lasso_tune <- tune_grid(
  lasso_wf,
  resamples = cross_validation_folds,
  grid = 50,  # éš¨æ©ŸæŠ½æ¨£ 50 çµ„çµ„åˆ
  metrics = metric_set(roc_auc, accuracy, precision, recall, f_meas),
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# 4. é¸æ“‡æœ€ä½³æ¨¡å‹
best_lasso <- select_best(lasso_tune, metric = "roc_auc")

# 5. Finalize workflow
final_lasso_wf <- finalize_workflow(lasso_wf, best_lasso)

# 6. æœ€çµ‚è©•ä¼°ï¼ˆlast fitï¼‰
final_lasso_fit <- last_fit(
  final_lasso_wf,
  split = data_split,
  metrics = metric_set(roc_auc, accuracy, precision, recall, f_meas)
)

# 7. æ”¶é›†æ¸¬è©¦é›†è¡¨ç¾
collect_metrics(final_lasso_fit)

```




# Elastic Net Logistic Regression - clean version

```{r}
# 1. Define model
elastic_spec <- logistic_reg(
  penalty = tune(),
  mixture = tune()    # Elastic Net: éœ€è¦åŒæ™‚èª¿å…©å€‹åƒæ•¸
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# 2. Create workflow
elastic_wf <- workflow() %>%
  add_model(elastic_spec) %>%
  add_recipe(logistic_recipe)

# 3. Tuning with grid = 50 (random)
elastic_tune <- tune_grid(
  elastic_wf,
  resamples = cross_validation_folds,
  grid = 50,
  metrics = metric_set(roc_auc, accuracy, recall, precision, f_meas),
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# 4. é¸å‡ºæœ€ä½³åƒæ•¸çµ„åˆ
best_elastic <- select_best(elastic_tune, metric = "roc_auc")

# 5. Finalize workflow
final_elastic_wf <- finalize_workflow(elastic_wf, best_elastic)

# 6. Last fit on test set
final_elastic_fit <- last_fit(
  final_elastic_wf,
  split = data_split,
  metrics = metric_set(roc_auc, accuracy, recall, precision, f_meas)
)

# 7. Collect final test performance
collect_metrics(final_elastic_fit)

```




# knn

```{r}

# 1. Define model specs
knn_spec <- nearest_neighbor(
  neighbors = tune(),
  dist_power = tune(),
  weight_func = tune()
) %>% 
  set_engine("kknn") %>% 
  set_mode("classification") 


# 2. Recipes
knn_rec <- recipe(attrition_flag ~ ., data = train_data) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_zv(all_predictors())

# Workflow
knn_wf <- workflow() %>%
  add_recipe(knn_rec) %>%
  add_model(knn_spec)

# KNN: tune_grid
knn_tune <- tune_grid(
  knn_wf,
  resamples = cross_validation_folds,
  grid = 20,
  metrics = metric_set(roc_auc, accuracy, recall, precision, f_meas),
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# é¸æ“‡æœ€ä½³çµ„åˆ
best_knn <- select_best(knn_tune, metric = "roc_auc")

# finalize workflow
final_knn_wf <- finalize_workflow(knn_wf, best_knn)

# æœ€çµ‚è©•ä¼°
final_knn_fit <- last_fit(
  final_knn_wf,
  split = data_split,
  metrics = metric_set(roc_auc, accuracy, precision, recall, f_meas)
)

# è©•ä¼°æŒ‡æ¨™
collect_metrics(final_knn_fit)

```


```{r}

saveRDS(knn_tune, file = "knn_tune_result.rds")

```

```{r}
rf_tune <- readRDS("rf_tune_result.rds")

```


# RF

```{r}
# Recipe for Random Forestï¼ˆä¸éœ€è¦ normalize/dummyï¼‰
rf_rec <- recipe(attrition_flag ~ ., data = train_data) %>%        step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# Model spec for RF
rf_spec <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

# Workflow
rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_spec)


# RF: tune_grid
rf_tune <- tune_grid(
  rf_wf,
  resamples = cross_validation_folds,
  grid = 50,
  metrics = metric_set(roc_auc, accuracy, recall, precision, f_meas),
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# é¸æ“‡æœ€ä½³åƒæ•¸
best_rf <- select_best(rf_tune, metric = "roc_auc")

# finalize workflow
final_rf_wf <- finalize_workflow(rf_wf, best_rf)

# æœ€çµ‚è©•ä¼°
final_rf_fit <- last_fit(
  final_rf_wf,
  split = data_split,
  metrics = lm_metric
)

# è©•ä¼°æŒ‡æ¨™
collect_metrics(final_rf_fit)



```





```{r}
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
```



#XGB

```{r}
# 1. Define model spec
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# 2. Define recipe
xgb_rec <- recipe(attrition_flag ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# 3. Create workflow
xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(xgb_rec)

# 4. Tuning grid
xgb_tune <- tune_grid(
  xgb_wf,
  resamples = cross_validation_folds,
  grid = 50,
  metrics = lm_metric,  # åŒä½ å…¶ä»–æ¨¡å‹çš„ metric set
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# 5. Select best
best_xgb <- select_best(xgb_tune, metric = "roc_auc")

# 6. Finalize
final_xgb_wf <- finalize_workflow(xgb_wf, best_xgb)

# 7. Last fit
final_xgb_fit <- last_fit(
  final_xgb_wf,
  split = data_split,
  metrics = lm_metric
)

# 8. Collect performance
collect_metrics(final_xgb_fit)

```





# f

```{r}
# 1. Define model specs
logistic_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

ridge_spec <- logistic_reg(penalty = tune(), mixture = 0) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

elastic_spec <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# 2. Define recipe (standardized)
logistic_recipe <- recipe(attrition_flag ~ ., data = train_data) %>%
  step_rm(clientnum) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# 3. Workflow set for logistic family
lm_wfset <- workflow_set(
  preproc = list(
    std_logistic = logistic_recipe,
    std_ridge = logistic_recipe,
    std_lasso = logistic_recipe,
    std_elastic = logistic_recipe
  ),
  models = list(
    logistic = logistic_spec,
    ridge = ridge_spec,
    lasso = lasso_spec,
    elastic = elastic_spec
  )
)

# 4. Metrics
lm_metric <- metric_set(accuracy, sensitivity, specificity, 
                        precision, recall, bal_accuracy, f_meas,
                        roc_auc, pr_auc)

# 5. Run tuning
lm_res <- lm_wfset %>% 
  workflow_map(
    "tune_grid",
    seed = 47969938,
    resamples = cross_validation_folds,
    grid = 50,
    metrics = lm_metric,
    control = control_grid(verbose = TRUE)
  )


# NONLINEAR MODELS -----------------------------------------------------------

# 1. Define model specs
knn_spec <- nearest_neighbor(
  neighbors = tune(),
  dist_power = tune(),
  weight_func = tune()
) %>% 
  set_engine("kknn") %>% 
  set_mode("classification") 

rf_spec <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

# 2. Recipes
knn_rec <- recipe(attrition_flag ~ ., data = train_data) %>% 
  step_rm(clientnum) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_zv(all_predictors())

rf_rec <- recipe(attrition_flag ~ ., data = train_data) %>% 
  step_rm(clientnum) %>%
  step_zv(all_predictors())

# 3. Combine into individual workflows
knn_wf <- workflow() %>%
  add_recipe(knn_rec) %>%
  add_model(knn_spec)

rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_spec)

# 4. Convert to workflow set + combine with logistic models
nonlinear_wfset <- as_workflow_set(knn = knn_wf, rf = rf_wf)
wf_set_combined <- bind_rows(nonlinear_wfset, lm_wfset)

# 5. Tune all
final_res <- wf_set_combined %>%
  workflow_map(
    "tune_grid",
    seed = 123,
    resamples = cross_validation_folds,  # same folds used for all
    grid = 50,
    metrics = lm_metric,
    control = control_grid(verbose = TRUE)
  )

# 6. Rank best by a key metric (e.g., roc_auc)
final_res %>%
  rank_results(select_best = TRUE) %>%
  filter(.metric == "roc_auc")
```













