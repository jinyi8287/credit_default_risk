---
title: "bank v3"
format: html
editor: visual
---

## Introduction
Credit card churn poses a critical threat to the profitability and long-term growth of retail banks. Credit card operations can contribute up to 25% of a bank’s retail revenue, driven by interest income, interchange fees, annual charges, and cross-selling opportunities for other financial products and services (McKinsey, 2023). Therefore, a high churn rate not only implies a direct revenue loss but may also indicate declining customer satisfaction and loyalty.

For Tifosi Bank, a recent surge in credit card attrition highlights the urgency of understanding and addressing this issue. Identifying at-risk customers and uncovering the underlying drivers of churn can help inform targeted retention strategies and operational improvements.

This report applies predictive modeling techniques to analyze customer churn at Tifosi Bank. By evaluating both behavioral and demographic indicators, we aim to uncover key patterns associated with attrition and develop a robust classification model to support proactive customer retention efforts.





Credit card churn poses a significant threat to the profitability, customer lifetime value, and strategic growth of retail banks. With credit card operations contributing up to 25% of a bank’s retail revenue—driven by interest income, interchange fees, annual charges, and cross-selling opportunities (McKinsey, 2023)—a rising churn rate represents not just a revenue loss but a potential erosion of customer trust and competitive positioning.

For Tifosi Bank, a recent spike in credit card attrition raises concerns about declining engagement and highlights the need for timely intervention. Left unaddressed, this trend may increase acquisition costs, weaken cross-sell pipelines, and undermine long-term market share. Identifying high-risk segments and understanding the key behavioral or demographic drivers of churn is essential to enable targeted, cost-effective retention strategies.

This report applies predictive modeling techniques to develop a churn classification framework tailored to Tifosi Bank’s credit card portfolio. By leveraging machine learning on behavioral and demographic data, we aim to provide actionable insights that inform retention planning, resource allocation, and customer experience management—ultimately helping leadership mitigate risk and sustain growth in a highly competitive market.

## Data Preprocessing

| Category              | Description                                                                 |
|-----------------------|-----------------------------------------------------------------------------|
| **Objective**         | Prepare features for modeling by cleaning and transforming variables.       |
| **Numerical Features**| - Removed ID column `clientnum` <br> - Applied standardization (`step_normalize`) |
| **Categorical Features**| - Converted all categorical predictors to dummy variables (`step_dummy`) <br> - Removed predictors with no variance (`step_zv`) |
| **Missing Values**    | - No missing values in the dataset <br> - No imputation needed |
| **Data Split**        | - Stratified 80/20 split by `attrition_flag` <br> - Ensured class balance |




你分析的整體流程（例如：資料清理、模型選擇、如何驗證）

你為什麼這樣做（例如：為了預測流失、找出主要驅動因子）

這樣的流程怎麼幫助銀行做出決策（例如：精準鎖定高風險客戶）



| 評分要求                                  | 實踐方式                                                          |
| ------------------------------------- | ------------------------------------------------------------- |
| Exceptionally clear and succinct      | 刪掉所有「技術術語」如 `glmnet`、`tune_grid`，改用「我們使用了一種經常應用於客戶流失預測的建模方式…」 |
| Effectively simplifies the process    | 用白話描述，例如：「我們先觀察了哪些客戶最常流失，接著利用多種模型測試預測效果…」                     |
| Tailored for a non-technical audience | 假設你的讀者是高層或客戶經理，說「為了讓行銷部門能快速辨識風險客戶，我們建立了一個簡單且準確的模型…」           |





# Analytical Overview


We began by examining the dataset to understand its structure and ensure data quality. This included identifying variable types, checking for missing values, and confirming that the engineered features—such as transaction changes over time—were valid and meaningful. We then conducted exploratory analysis to examine how churn rates varied across customer subgroups, such as card types, age brackets, and income levels, providing an early sense of the patterns behind attrition.

Following this, we developed a series of predictive models—tools that estimate the likelihood of a customer churning based on past patterns. These models learn from the behavior and characteristics of previous customers who left the bank and apply that knowledge to identify others who may be at risk. To capture different types of patterns in the data, we applied both linear models (e.g., logistic regression) and non-linear models (e.g., K-Nearest Neighbors and Random Forest).

Each model’s performance was assessed using a set of industry-standard metrics, including accuracy, ROC AUC, precision, recall, and F1 score. While accuracy shows the overall proportion of correct predictions, it can be misleading when most customers do not churn. Therefore, we prioritized recall—our ability to correctly flag actual churners—since missing those customers could mean significant revenue loss. At the same time, we monitored precision to ensure we didn’t over-alert and waste resources on customers unlikely to churn.

To balance both concerns, we used the F1 score as a key metric that summarizes how well the model manages this trade-off.

Based on the best-performing model, we identified the most influential factors driving churn. These insights directly inform business strategy and suggest targeted actions Tifosi Bank can take to reduce customer attrition and improve long-term loyalty.


We began by examining the dataset to understand the structure and ensure data quality, including identifying variable types and checking for any missing values. Exploratory analysis was conducted to investigate the distribution of churn across different customer groups—such as card types, age brackets, and income levels—to uncover early patterns in customer attrition.

eda

```{r}
library(DiagrammeR)

grViz("
digraph analysis_flow {
  graph [layout = dot, rankdir = LR]
  
  node [shape = box, style = filled, color = lightblue]

  A [label = 'Raw Data']
  B [label = 'Data Cleaning\n(Missing values,\nData types)']
  C [label = 'Exploratory Data Analysis\n(EDA)']
  D [label = 'Modeling\n(Logistic, KNN, RF, etc.)']
  E [label = 'Model Evaluation\n(ROC, Recall, F1)']
  F [label = 'Key Findings & Insights']
  G [label = 'Business Strategy\nRecommendations']

  A -> B -> C -> D -> E -> F -> G
}
")

```



```{r}
library(DiagrammeR)

grViz("
digraph tifosi_churn_flow {
  graph [layout = dot, rankdir = LR]
  node [shape = box, style = filled, color = lightblue, fontname = Helvetica]

  A [label = '1. Data Input\n(bank_churners.csv)']
  B [label = '2. Data Cleaning\n(Remove ID, Check NAs)']
  C [label = '3. EDA\n(Churn rate by group,\nBoxplots, Barcharts)']
  D [label = '4. Feature Engineering\n(Dummy, Normalize, Step_zv)']
  E [label = '5. Model Building\n(Logistic, KNN, RF, etc.)']
  F [label = '6. Cross-Validation\n(Grid Search, ROC, Recall)']
  G [label = '7. Model Evaluation\n(F1, Recall, Precision)']
  H [label = '8. Key Drivers\n(VIP, Interpretation)']
  I [label = '9. Business Actions\n(Retention Strategy, Targeting)']

  A -> B -> C -> D -> E -> F -> G -> H -> I
}
")
```






 (1) 清楚標出每一項「關鍵發現」（Key Insights）
哪些變數（特徵）是預測流失的主因？（使用 VIP 條狀圖）

哪些群體流失率最高？（用分組柱狀圖或 boxplot）

模型哪個最好？為什麼？（放一張性能指標比較表或雷達圖）

➡ 每個圖表旁邊都要寫「洞察結論」一句兩句，例如：

✅ Customers with low Total_Trans_Ct and high Avg_Utilization_Ratio are significantly more likely to churn.

📌 (2) 讓圖表直觀、美觀、清楚
不要複雜的 ggplot 預設灰背景

加大字體，加 title / caption

使用 color palette 區分類別，例如 churn vs non-churn

📊 建議圖表類型（配合你目前分析）：

VIP 變數重要性圖

模型比較雷達圖（f_meas, recall 為軸）

客戶群體 vs 流失率的柱狀圖

Confusion matrix / ROC curve（若簡單明瞭）

📌 (3) 有邏輯地串連故事（Storytelling）
可依照這個邏輯排列：

🔍 Who is churning?

📈 What behaviors predict churn?

⚙️ Which model performs best?

🎯 What actions can be taken?

這樣每段圖表分析都變成 故事的一部分，而不是「孤島」。






```{r,message=FALSE,warning=FALSE}
#package
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(here)
library(readr)
tidymodels_prefer()
```



```{r,message=FALSE}
#load in data
raw_data<-read_csv(here("Data","bank_churners.csv")) %>% 
  clean_names()

```


```{r}
skim(raw_data)
raw_data

names(raw_data)
```



```{r}
# 3. 檢查 Attrition_Flag 分布
raw_data %>%
  count(attrition_flag) %>%
  mutate(prop = n / sum(n))
```

#### variable description

| 變數名稱（英文）                   | 中文解釋            | 說明與建模觀察                                         |
| -------------------------- | --------------- | ----------------------------------------------- |
| `clientnum`                | 客戶編號            | 唯一識別碼，建模時不會使用（唯一 ID）                            |
| `attrition_flag`           | 客戶是否流失          | 目標變數：1 = 流失，0 = 留存                              |
| `customer_age`             | 客戶年齡            | 數值變數，可能與信用使用行為或忠誠度相關                            |
| `gender`                   | 性別              | 類別變數，"M" 或 "F"，可視情況保留或轉換為 dummy                 |
| `dependent_count`          | 扶養人數            | 整數，可能與支出能力與忠誠度相關                                |
| `education_level`          | 教育程度            | 類別變數，如 Graduate、Post-Graduate、High School 等     |
| `marital_status`           | 婚姻狀態            | 類別變數，可能影響財務行為與穩定性                               |
| `income_category`          | 年收入範圍           | 類別變數（\$40K–\$60K、\$60K–\$80K 等），建議保留            |
| `card_category`            | 信用卡類型           | 類別變數（Blue、Silver、Gold、Platinum），可探索是否與 churn 有關 |
| `months_on_book`           | 開戶月份數           | 客戶與銀行關係長度（忠誠度 proxy）                            |
| `total_relationship_count` | 產品數量            | 客戶與銀行的關係深度（帳戶數、產品數等）                            |
| `months_inactive_12_mon`   | 最近一年內的閒置月份數     | 客戶閒置程度，與流失高度相關的變數                               |
| `contacts_count_12_mon`    | 最近一年內的聯絡次數      | 與客服互動次數，可反映滿意度或問題                               |
| `credit_limit`             | 信用卡額度           | 客戶核定的信用額度，與信用評級和風險管理相關                          |
| `total_revolving_bal`      | 循環信用餘額          | 未償還金額，可能代表風險或活躍度                                |
| `avg_open_to_buy`          | 平均可用信用額度        | 可用信用 = 信用額度 - 餘額，代表可支配空間                        |
| `total_amt_chng_q4_q1`     | 總交易金額變動率（Q1→Q4） | 表示交易金額的變動趨勢，是行為變化的指標                            |
| `total_trans_amt`          | 年度總交易金額         | 客戶活躍程度（整體使用情況）                                  |
| `total_trans_ct`           | 年度總交易筆數         | 另一個活躍指標，與上面搭配使用                                 |
| `total_ct_chng_q4_q1`      | 總交易筆數變動率（Q1→Q4） | 看交易次數是否明顯下降或上升                                  |
| `avg_utilization_ratio`    | 信用使用率           | 信用卡餘額佔額度比率，高者可能代表風險或頻繁使用者                       |





# eda




#### 哪些族群 churn rate 高？

###### churn rate in every income level

```{r}
# 查看類別順序
unique(raw_data$income_category)

# 建立交叉表 + 百分比
income_tab <- raw_data %>%
  tabyl(income_category, attrition_flag) %>%
  adorn_percentages("row") %>%
  adorn_totals("row") %>%
  adorn_pct_formatting(digits = 1)

# 長格式資料
income_long <- income_tab %>%
  filter(income_category != "Total") %>%
  pivot_longer(cols = -income_category, names_to = "attrition_flag", values_to = "percent")

# 計算實際筆數
income_counts <- raw_data %>%
  count(income_category, attrition_flag)

# 合併資料
income_plot_data <- left_join(income_counts, income_long, by = c("income_category", "attrition_flag"))

# 設定順序
income_plot_data <- income_plot_data %>%
  mutate(income_category = fct_relevel(income_category,
    "< $40K", "$40K - $60K", "$60K - $80K", "$80K - $120K", "> $120K", "Unknown"
  ))

# 繪圖
ggplot(income_plot_data, aes(x = income_category, y = n, fill = attrition_flag)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = percent),
            position = position_dodge(width = 0.9), vjust = -0.3, size = 4) +
  scale_fill_manual(values = c("#004c6d", "#a7c6ed")) +
  labs(
    title = "Customer Churn by Income Level",
    subtitle = "Based on 10,127 credit card customers",
    x = "Income Category",
    y = "Number of Customers"
  ) +
  theme_minimal() +
  theme(legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

```


```{r}
raw_data %>%
  count(income_category) %>%
  mutate(prop = n / sum(n)) %>%
  filter(income_category == "Unknown")

```


```{r}

income_churn <- raw_data %>%
  mutate(income_category = as.factor(income_category)) %>%  # 先轉 factor
  group_by(income_category) %>%
  summarise(
    churn_rate = mean(attrition_flag == "Attrited Customer"),
    total = n()
  ) %>%
  mutate(income_category = fct_relevel(
    income_category,
    "Unknown", 
    "Less than $40K", 
    "$40K - $60K", 
    "$60K - $80K", 
    "$80K - $120K", 
    "$120K +"
  ))


ggplot(income_churn, aes(x = income_category, y = churn_rate)) +
  geom_col(fill = "#D72638") +
  geom_text(aes(label = scales::percent(churn_rate, accuracy = 0.1)), vjust = -0.3, size = 4) +
  labs(
    title = "Customer Churn Rate by Income Category",
    subtitle = "Proportion of customers who attrited in each income group",
    x = "Income Category",
    y = "Churn Rate (%)"
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )


```

###### churn rate in each education level


#### churn rate in each card category


```{r}
raw_data %>%
  count(card_category) %>%
  mutate(prop = n / sum(n)) 
```


```{r,message=FALSE,echo=FALSE}
unique(raw_data$card_category)

# 交叉表 + 百分比
card_tab <- raw_data %>%
  tabyl(card_category, attrition_flag) %>%
  adorn_percentages("row") %>%
  adorn_totals("row") %>%
  adorn_pct_formatting(digits = 1)

card_long <- card_tab %>%
  filter(card_category != "Total") %>%
  pivot_longer(cols = -card_category, names_to = "attrition_flag", values_to = "percent")


# 計算實際筆數
card_counts <- raw_data %>%
  count(card_category, attrition_flag)

#合併為繪圖資料框
card_plot_data <- left_join(card_counts, card_long, by = c("card_category", "attrition_flag"))

#排序
library(forcats)
card_plot_data <- card_plot_data %>%
  mutate(card_category = fct_relevel(card_category, "Blue", "Silver", "Gold", "Platinum"))


# 繪圖
ggplot(card_plot_data, aes(x = card_category, y = n, fill = attrition_flag)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = percent),
            position = position_dodge(width = 0.9), vjust = -0.3, size = 4) +
  scale_fill_manual(values = c("#004c6d", "#a7c6ed")) +
  labs(
    title = "Customer Churn by Credit Card Category",
    subtitle = "Based on 10,127 credit card customers",
    x = "Card Category",
    y = "Number of Customers"
  ) +
  theme_minimal() +
  theme(legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

```




```{r}
# Step 1: 計算每類卡別的流失率與總人數
card_churn <- raw_data %>%
  mutate(card_category = as.factor(card_category)) %>%
  group_by(card_category) %>%
  summarise(
    churn_rate = mean(attrition_flag == "Attrited Customer"),
    total = n()
  ) %>%
  mutate(card_category = fct_relevel(
    card_category,
    "Blue", "Silver", "Gold", "Platinum"  # 可調整順序，如要 Blue 在前
  ))

# Step 2: 繪圖
ggplot(card_churn, aes(x = card_category, y = churn_rate)) +
  geom_col(fill = "#5D3FD3") +
  geom_text(aes(label = scales::percent(churn_rate, accuracy = 0.1)), 
            vjust = -0.3, size = 4) +
  labs(
    title = "Customer Churn Rate by Card Category",
    subtitle = "Higher churn observed among Gold and Platinum cardholders",
    x = "Card Category",
    y = "Churn Rate (%)"
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )

```


交易次数对比（原文第9/26页）

推荐形式：背靠背箱线图（attrited vs existing左右对比）

优化建议：

添加参考线标注行业平均水平

标题改为："流失客户年均交易次数减少XX%"

信用额度使用率（原文第10页）

推荐形式：双Y轴组合图（左边箱线图+右边流失率曲线）

价值点：展示"使用率低于20%的客户流失风险增加XX%"

账户不活跃月份数（原文12-13/18页）

推荐形式：热力图（横轴：不活跃月份数，纵轴：流失率）

改进建议：标注关键阈值"3个月不活跃流失风险翻倍"



## modeling

```{r,message=FALSE}
#package
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(here)
library(readr)
tidymodels_prefer()

raw_data<-read_csv(here("Data","bank_churners.csv")) %>% 
  clean_names() %>% 
  mutate(across(where(is.character), as_factor)) %>% 
  select(-clientnum)

```


# Data split

```{r}
set.seed(47969938)
data_split<- initial_split(raw_data, prop = 0.8, strata = attrition_flag)
train_data<- training(data_split)
test_data<- testing(data_split)
# Cross-validation folds
cross_validation_folds <- vfold_cv(train_data, v = 10, strata = attrition_flag)

```

## Logistic baseline

```{r}
# Define model
logistic_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Define recipe ---------------------------------------------------------------
logistic_recipe <- recipe(attrition_flag ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%            
  step_zv(all_predictors()) %>%                        
  step_normalize(all_numeric_predictors()) 


# Create workflow -------------------------------------------------------------
logistic_wf <- workflow() |> 
  add_model(logistic_spec) |> 
  add_recipe(logistic_recipe)

# Cross-validation: fit_resamples ---------------------------------------------
logistic_cv <- fit_resamples(
  logistic_wf,
  resamples = cross_validation_folds,
  metrics = metric_set(roc_auc, accuracy, precision, recall, f_meas),
  control = control_resamples(
    save_pred = TRUE,
    event_level = "second"
  )
)

# Evaluation on cross-validation folds ----------------------------------------
collect_metrics(logistic_cv)

# Last fit: 用 train_data2 fit，predict test_data2
final_logistic_fit <- last_fit(
  logistic_wf,
  split = data_split,
  metrics = metric_set( 
    roc_auc, 
    accuracy,
    recall,     
    precision,  
    f_meas      
))

# Collect performance on test set
collect_metrics(final_logistic_fit)

```

```{r}
logistic_preds <- collect_predictions(final_logistic_fit)
logistic_preds %>%
  conf_mat(truth = attrition_flag, estimate = .pred_class) %>%
  summary()

```



# Ridge Logistic Regression (mixture = 0) 


```{r}
# 1. Define model spec
ridge_spec <- logistic_reg(
  penalty = tune(),
  mixture = 0
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# 2. Create workflow
ridge_wf <- workflow() %>%
  add_model(ridge_spec) %>%
  add_recipe(logistic_recipe)

# 3. Define tuning grid
ridge_grid <- grid_regular(
  penalty(range = c(-4, 0)),
  levels = 30
)

# 4. Cross-validation tuning
ridge_tune <- tune_grid(
  ridge_wf,
  resamples = cross_validation_folds,  # 例如 vfold_cv(train_data, v = 10)
  grid = 50,
  metrics = metric_set(roc_auc, accuracy, precision, recall, f_meas),
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# 5. Select best model
best_ridge <- select_best(ridge_tune, metric = "roc_auc")

# 6. Finalize workflow
final_ridge_wf <- finalize_workflow(ridge_wf, best_ridge)

# 7. Last fit
final_ridge_fit <- last_fit(
  final_ridge_wf,
  split = data_split,
  metrics = metric_set(
    roc_auc, 
    accuracy,
    recall,
    precision,
    f_meas
  )
)

# 8. Collect test performance
collect_metrics(final_ridge_fit)


```






# Lasso Logistic Regression (mixture = 1) 


```{r}
# 1. Define model
lasso_spec <- logistic_reg(
  penalty = tune(),
  mixture = 1  # Lasso
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# 2. Create workflow
lasso_wf <- workflow() %>%
  add_model(lasso_spec) %>%
  add_recipe(logistic_recipe)  # 保留你原本的乾淨 recipe

# 3. Cross-validation tuning (grid = 50 random)
lasso_tune <- tune_grid(
  lasso_wf,
  resamples = cross_validation_folds,
  grid = 50,  # 隨機抽樣 50 組組合
  metrics = metric_set(roc_auc, accuracy, precision, recall, f_meas),
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# 4. 選擇最佳模型
best_lasso <- select_best(lasso_tune, metric = "roc_auc")

# 5. Finalize workflow
final_lasso_wf <- finalize_workflow(lasso_wf, best_lasso)

# 6. 最終評估（last fit）
final_lasso_fit <- last_fit(
  final_lasso_wf,
  split = data_split,
  metrics = metric_set(roc_auc, accuracy, precision, recall, f_meas)
)

# 7. 收集測試集表現
collect_metrics(final_lasso_fit)

```




# Elastic Net Logistic Regression - clean version

```{r}
# 1. Define model
elastic_spec <- logistic_reg(
  penalty = tune(),
  mixture = tune()    # Elastic Net: 需要同時調兩個參數
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# 2. Create workflow
elastic_wf <- workflow() %>%
  add_model(elastic_spec) %>%
  add_recipe(logistic_recipe)

# 3. Tuning with grid = 50 (random)
elastic_tune <- tune_grid(
  elastic_wf,
  resamples = cross_validation_folds,
  grid = 50,
  metrics = metric_set(roc_auc, accuracy, recall, precision, f_meas),
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# 4. 選出最佳參數組合
best_elastic <- select_best(elastic_tune, metric = "roc_auc")

# 5. Finalize workflow
final_elastic_wf <- finalize_workflow(elastic_wf, best_elastic)

# 6. Last fit on test set
final_elastic_fit <- last_fit(
  final_elastic_wf,
  split = data_split,
  metrics = metric_set(roc_auc, accuracy, recall, precision, f_meas)
)

# 7. Collect final test performance
collect_metrics(final_elastic_fit)

```




# knn

```{r}

# 1. Define model specs
knn_spec <- nearest_neighbor(
  neighbors = tune(),
  dist_power = tune(),
  weight_func = tune()
) %>% 
  set_engine("kknn") %>% 
  set_mode("classification") 


# 2. Recipes
knn_rec <- recipe(attrition_flag ~ ., data = train_data) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_zv(all_predictors())

# Workflow
knn_wf <- workflow() %>%
  add_recipe(knn_rec) %>%
  add_model(knn_spec)

# KNN: tune_grid
knn_tune <- tune_grid(
  knn_wf,
  resamples = cross_validation_folds,
  grid = 20,
  metrics = metric_set(roc_auc, accuracy, recall, precision, f_meas),
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# 選擇最佳組合
best_knn <- select_best(knn_tune, metric = "roc_auc")

# finalize workflow
final_knn_wf <- finalize_workflow(knn_wf, best_knn)

# 最終評估
final_knn_fit <- last_fit(
  final_knn_wf,
  split = data_split,
  metrics = metric_set(roc_auc, accuracy, precision, recall, f_meas)
)

# 評估指標
collect_metrics(final_knn_fit)

```


```{r}

saveRDS(knn_tune, file = "knn_tune_result.rds")

```

```{r}
rf_tune <- readRDS("rf_tune_result.rds")

```


# RF

```{r}
# Recipe for Random Forest（不需要 normalize/dummy）
rf_rec <- recipe(attrition_flag ~ ., data = train_data) %>%        step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# Model spec for RF
rf_spec <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

# Workflow
rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_spec)


# RF: tune_grid
rf_tune <- tune_grid(
  rf_wf,
  resamples = cross_validation_folds,
  grid = 50,
  metrics = metric_set(roc_auc, accuracy, recall, precision, f_meas),
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# 選擇最佳參數
best_rf <- select_best(rf_tune, metric = "roc_auc")

# finalize workflow
final_rf_wf <- finalize_workflow(rf_wf, best_rf)

# 最終評估
final_rf_fit <- last_fit(
  final_rf_wf,
  split = data_split,
  metrics = lm_metric
)

# 評估指標
collect_metrics(final_rf_fit)



```





```{r}
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
```



#XGB

```{r}
# 1. Define model spec
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# 2. Define recipe
xgb_rec <- recipe(attrition_flag ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# 3. Create workflow
xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(xgb_rec)

# 4. Tuning grid
xgb_tune <- tune_grid(
  xgb_wf,
  resamples = cross_validation_folds,
  grid = 50,
  metrics = lm_metric,  # 同你其他模型的 metric set
  control = control_grid(
    save_pred = TRUE,
    event_level = "second",
    verbose = TRUE
  )
)

# 5. Select best
best_xgb <- select_best(xgb_tune, metric = "roc_auc")

# 6. Finalize
final_xgb_wf <- finalize_workflow(xgb_wf, best_xgb)

# 7. Last fit
final_xgb_fit <- last_fit(
  final_xgb_wf,
  split = data_split,
  metrics = lm_metric
)

# 8. Collect performance
collect_metrics(final_xgb_fit)

```





# f

```{r}
# 1. Define model specs
logistic_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

ridge_spec <- logistic_reg(penalty = tune(), mixture = 0) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

elastic_spec <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# 2. Define recipe (standardized)
logistic_recipe <- recipe(attrition_flag ~ ., data = train_data) %>%
  step_rm(clientnum) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# 3. Workflow set for logistic family
lm_wfset <- workflow_set(
  preproc = list(
    std_logistic = logistic_recipe,
    std_ridge = logistic_recipe,
    std_lasso = logistic_recipe,
    std_elastic = logistic_recipe
  ),
  models = list(
    logistic = logistic_spec,
    ridge = ridge_spec,
    lasso = lasso_spec,
    elastic = elastic_spec
  )
)

# 4. Metrics
lm_metric <- metric_set(accuracy, sensitivity, specificity, 
                        precision, recall, bal_accuracy, f_meas,
                        roc_auc, pr_auc)

# 5. Run tuning
lm_res <- lm_wfset %>% 
  workflow_map(
    "tune_grid",
    seed = 47969938,
    resamples = cross_validation_folds,
    grid = 50,
    metrics = lm_metric,
    control = control_grid(verbose = TRUE)
  )


# NONLINEAR MODELS -----------------------------------------------------------

# 1. Define model specs
knn_spec <- nearest_neighbor(
  neighbors = tune(),
  dist_power = tune(),
  weight_func = tune()
) %>% 
  set_engine("kknn") %>% 
  set_mode("classification") 

rf_spec <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

# 2. Recipes
knn_rec <- recipe(attrition_flag ~ ., data = train_data) %>% 
  step_rm(clientnum) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_zv(all_predictors())

rf_rec <- recipe(attrition_flag ~ ., data = train_data) %>% 
  step_rm(clientnum) %>%
  step_zv(all_predictors())

# 3. Combine into individual workflows
knn_wf <- workflow() %>%
  add_recipe(knn_rec) %>%
  add_model(knn_spec)

rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_spec)

# 4. Convert to workflow set + combine with logistic models
nonlinear_wfset <- as_workflow_set(knn = knn_wf, rf = rf_wf)
wf_set_combined <- bind_rows(nonlinear_wfset, lm_wfset)

# 5. Tune all
final_res <- wf_set_combined %>%
  workflow_map(
    "tune_grid",
    seed = 123,
    resamples = cross_validation_folds,  # same folds used for all
    grid = 50,
    metrics = lm_metric,
    control = control_grid(verbose = TRUE)
  )

# 6. Rank best by a key metric (e.g., roc_auc)
final_res %>%
  rank_results(select_best = TRUE) %>%
  filter(.metric == "roc_auc")
```













