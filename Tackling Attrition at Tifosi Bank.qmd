---
title: "Tackling Attrition at Tifosi Bank"
format: pdf
editor: visual
---




```{r,message=FALSE,warning=FALSE}
#package
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(here)
library(readr)
tidymodels_prefer()
```



```{r,message=FALSE}
data<-read_csv(here("Data","bank_churners.csv")) %>% 
  clean_names()

```

```{r}
data
```



```{r}
skim(data)
```




## Convert categorical variables to factors where necessary


```{r}
#sapply()：是 R 裡用來「逐一作用到每個元素」的函數
names(data)[sapply(data, is.character)]

unique(data$attrition_flag)
unique(data$gender)
unique(data$education_level)
unique(data$marital_status)
unique(data$income_category)
unique(data$card_category)


data <- data %>% 
  mutate(
    attrition_flag = as.factor(attrition_flag),
    gender = as.factor(gender),
    education_level = as.factor(education_level),
    marital_status = as.factor(marital_status),
    income_category = as.factor(income_category),
    card_category = as.factor(card_category)
  )


```



## eda

#### missing value

```{r}
library(naniar)

miss_var_summary(data)
colSums(is.na(data))

```


#### factor(merging or not)

```{r,warning=FALSE,message=FALSE}
library(ggplot2)


data %>% 
  ggplot(aes(x = education_level)) +
  geom_bar(fill = "skyblue") +
  coord_flip() +  # 翻轉座標，類別名不會擠在一起
  labs(title = "Education Level Distribution",
       x = "Education Level",
       y = "Count")



library(dplyr)
library(forcats)

# 先計算每個 level 的比例
level_summary <- data %>%
  count(education_level) %>%
  mutate(prop = n / sum(n))  #	算出每個類別佔全部樣本的比例 

# 決定閾值（比如5%以下算 rare）
threshold <- 0.05

# 標記 rare / common
data <- data %>%
  left_join(level_summary, by = "education_level") %>%
  mutate(
    small_level = if_else(prop < threshold, "Rare", "Common")
  ) #把 level_summary 的 n 和 prop 合併到 data，以 education_level 為鍵

data %>%
  ggplot(aes(x = fct_infreq(education_level), fill = small_level)) +
  geom_bar() +
  coord_flip() +
  labs(title = "Education Level Distribution Highlighting Rare Levels",
       x = "Education Level",
       y = "Count") +
  scale_fill_manual(values = c("Common" = "skyblue", "Rare" = "red")) +
  theme_minimal()

```




```{r}
library(ggplot2)
library(dplyr)

# 指定要畫的變數
factor_vars <- c("attrition_flag", "gender", "education_level", 
                 "marital_status", "income_category", "card_category")

# 用 for loop 自動畫每個
for (var in factor_vars) {
  
  data %>%
    ggplot(aes_string(x = var)) +
    geom_bar(fill = "skyblue") +
    coord_flip() +
    labs(
      title = paste("Distribution of", var),
      x = var,
      y = "Count"
    ) +
    theme_minimal() -> p
  
  print(p)
}

```



#### response variable 

```{r,warning=FALSE}
library(ggplot2)

ggplot(data, aes(x = attrition_flag)) +
  geom_bar(fill = "skyblue") +
  geom_text(stat = "count", aes(label = after_stat(count)), hjust = 0.2) + 
  coord_flip() +  #after_stat(count)
  labs(title = "Attrition Flag Distribution",
       x = "Customer Status",
       y = "Count") +
  theme_minimal()

  
```




#### relationships among the numerical variables

```{r,message=FALSE}
#library(GGally)

#data %>% 
  # select(where(is.numeric)) %>% 
  #GGally::ggpairs(upper = list(continuous = wrap("cor", size = 2)))

#Pearson correlation 測試線性強度
```


```{r}
library(dplyr)
library(tidyr)

# 設定你的數值變數資料（只選 numeric）
numeric_data <- data %>% select(where(is.numeric))

# 計算 correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# 把 correlation matrix 轉成長格式
cor_df <- as.data.frame(as.table(cor_matrix))

# 避免重複（只保留上三角）
cor_df <- cor_df %>%
  filter(as.character(Var1) < as.character(Var2)) 

# 篩選出高相關變數對
high_cor <- cor_df %>%
  filter(abs(Freq) > 0.7) %>%
  arrange(desc(abs(Freq)))

# 顯示結果
high_cor

```






```{r}
library(ggplot2)

data %>%
  ggplot(aes(x = education_level, fill = attrition_flag)) +
  geom_bar(position = "fill") +
  coord_flip() +
  labs(
    x = "Education Level",
    y = "Proportion",
    title = "Proportion of Attrition by Education Level"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("skyblue", "red")) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))


```


```{r}
library(ggplot2)
library(dplyr)
library(forcats)

# 先列出你要分析的 categorical predictors
factor_vars <- c("gender", "education_level", "marital_status", "income_category", "card_category")

# 重新整理資料：轉成長格式（方便畫 facet_wrap）
data_long <- data %>%
  select(all_of(factor_vars), attrition_flag) %>%
  pivot_longer(cols = -attrition_flag, names_to = "predictor", values_to = "level")

# 畫圖
ggplot(data_long, aes(x = level, fill = attrition_flag)) +
  geom_bar(position = "fill") +
  facet_wrap(~ predictor, scales = "free_x") +
  coord_flip() +
  labs(
    title = "Proportion of Attrition by Categorical Predictors",
    x = "Levels",
    y = "Proportion"
  ) +
  scale_fill_manual(values = c("skyblue", "red")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))

```

#### promising predictor

```{r}
# 做交叉表
tab <- table(data$education_level, data$attrition_flag)

# 做卡方檢定
stats::chisq.test(tab)

```

```{r}
library(dplyr)
library(purrr)

# 你的 categorical predictors
factor_vars <- c("gender", "education_level", "marital_status", "income_category", "card_category")

# 自動跑卡方檢定
chi_sq_results <- map_dfr(factor_vars, function(var) {
  
  tab <- table(data[[var]], data$attrition_flag)
  
  test_result <- stats::chisq.test(tab)
  
  tibble(
    predictor = var,
    p_value = test_result$p.value,
    statistic = test_result$statistic
  )
})

# 查看結果
chi_sq_results %>%
  arrange(p_value)

```
- 1
p-value < 0.05	拒絕虛無假設（有顯著關聯），這個 predictor 和 attrition_flag 有統計上顯著關係，是 promising predictor

p-value ≥ 0.05	無法拒絕虛無假設（沒顯著關聯），這個 predictor 和 attrition_flag 沒有明顯關係，不是重點 predictor

- 2
那「類別 vs 類別」關聯該用什麼統計檢定？

情境	適合用的檢定方法
數值 vs 數值	Pearson correlation (r)
數值 vs 類別	t-test, ANOVA
類別 vs 類別	卡方檢定 (Chi-squared test)


-3

Response類型	Predictor類型	方法
類別 (2類)	類別	卡方檢定（Chi-squared test）
類別 (2類)	數值	t檢定（t-test）
類別 (>2類)	數值	ANOVA（變異數分析）

# 需確認 logistic_recipe 中的 step_corr 是否在訓練集上計算相關性，避免數據洩漏  

```{r}
library(dplyr)
library(purrr)
library(broom) # 讓 t.test 結果整理成表格

# 選出數值型變數
numeric_vars <- data %>% 
  select(where(is.numeric)) %>% 
  names()

# 自動對每個數值變數跑 t-test
t_test_results <- map_dfr(numeric_vars, function(var) {
  
  # 跑 t-test
  test_result <- t.test(data[[var]] ~ data$attrition_flag)
  
  # 整理成一行 tibble
  tidy_result <- broom::tidy(test_result)
  
  tibble(
    predictor = var,
    p_value = tidy_result$p.value,
    statistic = tidy_result$statistic
  )
})

# 查看結果
t_test_results %>%
  arrange(p_value)

```


# data split

```{r}
set.seed(47969938)
data_split<- initial_split(data, prop = 0.8, strata = attrition_flag)
train_data<- training(data_split)
test_data <- testing(data_split)
# Cross-validation folds
cross_validation_folds <- vfold_cv(train_data, v = 10, strata = attrition_flag)

```


# logistic base model

```{r}
# Define model
base_logistic_spec <-logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")
# Define recipe---------------------------------------------------------------
logistic_recipe <- recipe(attrition_flag ~ ., data = train_data) |> 
  step_other(all_nominal_predictors(), threshold = 0.05) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_corr(threshold = 0.7) |> 
  step_normalize(all_numeric_predictors())

# Create workflow-------------------------------------------------------------
base_logistic_wf <- workflow() |> 
  add_model(base_logistic_spec) |> 
  add_recipe(logistic_recipe)
# Final model fitting---------------------------------------------------------
base_logistic_cv <- fit_resamples(
  base_logistic_wf,
  resamples = cross_validation_folds,
  metrics = metric_set(roc_auc, accuracy)
)


# Evaluation ------------------------------------
collect_metrics(base_logistic_cv)

# Last fit: 用 train_data fit，再 predict test_data
final_base_fit <- last_fit(
  base_logistic_wf,
  split = data_split,   # 直接用你之前的 initial_split物件
  metrics = metric_set(roc_auc, accuracy)
)

# Collect performance on test set
collect_metrics(final_base_fit)

```



#  Ridge Logistic Regression (mixture = 0)


```{r}
# Define model
ridge_spec <- logistic_reg(
  penalty = tune(),  # penalty要tune
  mixture = 0        # mixture = 0 = Ridge
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# Create workflow
ridge_wf <- workflow() %>%
  add_model(ridge_spec) %>%
  add_recipe(logistic_recipe)

# Define tuning grid
ridge_grid <- grid_regular(
  penalty(range = c(-4, 0)), 
  levels = 30
)

# Tuning
ridge_tune <- tune_grid(
  ridge_wf,
  resamples = cross_validation_folds,
  grid = ridge_grid,
  metrics = metric_set(roc_auc, accuracy)
)

# Select best model
best_ridge <- select_best(ridge_tune, metric = "roc_auc")

# Finalize workflow
final_ridge_wf <- finalize_workflow(ridge_wf, best_ridge)

# Last fit
final_ridge_fit <- last_fit(
  final_ridge_wf,
  split = data_split,
  metrics = metric_set(roc_auc, accuracy)
)

# Collect metrics
collect_metrics(final_ridge_fit)

```


# Lasso Logistic Regression (mixture = 1)

```{r}
# Define model
lasso_spec <- logistic_reg(
  penalty = tune(),  # penalty要tune
  mixture = 1        # mixture = 1 = Lasso
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# Create workflow
lasso_wf <- workflow() %>%
  add_model(lasso_spec) %>%
  add_recipe(logistic_recipe)

# Define tuning grid
lasso_grid <- grid_regular(
  penalty(range = c(-4, 0)),
  levels = 30
)

# Tuning
lasso_tune <- tune_grid(
  lasso_wf,
  resamples = cross_validation_folds,
  grid = lasso_grid,
  metrics = metric_set(roc_auc, accuracy)
)

# Select best model
best_lasso <- select_best(lasso_tune, metric = "roc_auc")

# Finalize workflow
final_lasso_wf <- finalize_workflow(lasso_wf, best_lasso)

# Last fit
final_lasso_fit <- last_fit(
  final_lasso_wf,
  split = data_split,
  metrics = metric_set(roc_auc, accuracy)
)

# Collect metrics
collect_metrics(final_lasso_fit)

```



# Ridge Logistic Regression

```{r}
# Define model
elastic_spec <- logistic_reg(
  penalty = tune(),
  mixture = tune()    # Elastic Net要tune mixture
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# Create workflow
elastic_wf <- workflow() %>%
  add_model(elastic_spec) %>%
  add_recipe(logistic_recipe)

# Define tuning grid
elastic_grid <- grid_regular(
  penalty(range = c(-4, 0)),
  mixture(range = c(0, 1)),    # 同時調整mixture (0=Ridge到1=Lasso)
  levels = c(20, 5)            # penalty 取20個點, mixture取5個點（可以調整）
)

# Tuning
elastic_tune <- tune_grid(
  elastic_wf,
  resamples = cross_validation_folds,
  grid = elastic_grid,
  metrics = metric_set(roc_auc, accuracy)
)

# Select best model
best_elastic <- select_best(elastic_tune, metric = "roc_auc")

# Finalize workflow
final_elastic_wf <- finalize_workflow(elastic_wf, best_elastic)

# Last fit
final_elastic_fit <- last_fit(
  final_elastic_wf,
  split = data_split,
  metrics = metric_set(roc_auc, accuracy)
)

# Collect metrics
collect_metrics(final_elastic_fit)


```




## Knn

```{r}
# 1.1 Define KNN model
knn_spec <- nearest_neighbor(
  neighbors = tune()
) %>%
  set_engine("kknn") %>%
  set_mode("classification")

# 1.2 Define Recipe（記得要normalize）
knn_recipe <- recipe(attrition_flag ~ ., data = train_data) %>%
  step_other(all_nominal_predictors(), threshold = 0.05) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_corr(threshold = 0.7) %>%
  step_normalize(all_numeric_predictors())

# 1.3 Create Workflow
knn_wf <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_recipe)

# 1.4 Define Grid
knn_grid <- grid_regular(
  neighbors(range = c(3, 50)),
  levels = 10
)

# 1.5 Tune
knn_tuned <- tune_grid(
  knn_wf,
  resamples = cross_validation_folds,
  grid = knn_grid,
  metrics = metric_set(roc_auc, accuracy)
)

# 1.6 Select best
knn_best <- select_best(knn_tuned, metric = "roc_auc")

# 1.7 Finalize
final_knn_wf <- finalize_workflow(knn_wf, knn_best)


knn_final_fit <- last_fit(
  final_knn_wf,
  split = data_split  # ← 這是你事先做好的 train/test split物件
)

collect_metrics(knn_final_fit)
```




## rf

```{r}
# 2.1 Define Random Forest model
rf_spec <- rand_forest(
  mtry = tune(),
  min_n = tune()
) %>%
  set_engine("ranger") %>%
  set_mode("classification")

# 2.2 Define Recipe（RF可以不用normalize）
rf_recipe <- recipe(attrition_flag ~ ., data = train_data) %>%
  step_other(all_nominal_predictors(), threshold = 0.05) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_corr(threshold = 0.7)

# 2.3 Create Workflow
rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(rf_recipe)

# 2.4 Define Grid
rf_grid <- grid_regular(
  mtry(range = c(2, 10)),
  min_n(range = c(5, 30)),
  levels = 5
)

# 2.5 Tune
rf_tuned <- tune_grid(
  rf_wf,
  resamples = cross_validation_folds,
  grid = rf_grid,
  metrics = metric_set(roc_auc, accuracy)
)

# 2.6 Select best
rf_best <- select_best(rf_tuned, metric = "roc_auc")

# 2.7 Finalize
final_rf_wf <- finalize_workflow(rf_wf, rf_best)

# Last fit
final_rf_fit <- last_fit(
  final_rf_wf,
  split = data_split,
  metrics = metric_set(roc_auc, accuracy)
)

# Collect metrics
collect_metrics(final_rf_fit)

```


Data Leakage?

```{r}
# 載入 yardstick 套件 (通常 tidymodels 裡已經有)
library(yardstick)

# 產生預測結果
rf_predictions <- predict(final_rf_fit$.workflow[[1]], test_data, type = "class") %>%
  bind_cols(test_data)

# 建立 Confusion Matrix
conf_mat(data = rf_predictions, 
         truth = attrition_flag, 
         estimate = .pred_class)

```


```{r}
# 快速查看attrition_flag在 test set中的分布
test_data %>%
  count(attrition_flag) %>%
  mutate(prop = n / sum(n))

```

```{r}
#library(vip)
#final_rf_fit %>% 
  #extract_fit_parsnip() %>% 
  #vip(num_features = 20)
```


## xboost

```{r}
# 3.1 Define XGBoost model
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# 3.2 Define Recipe（XGB也可以不用normalize）
xgb_recipe <- recipe(attrition_flag ~ ., data = train_data) %>%
  step_other(all_nominal_predictors(), threshold = 0.05) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_corr(threshold = 0.7)

# 3.3 Create Workflow
xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(xgb_recipe)

# 3.4 Define Grid
xgb_grid <- grid_latin_hypercube(
  trees(range = c(100, 1000)),
  tree_depth(range = c(2, 10)),
  learn_rate(range = c(0.01, 0.3)),
  loss_reduction(range = c(0.0001, 1)),
  sample_size = sample_prop(range = c(0.5, 1)),
  mtry(range = c(2, 10)),
  size = 20
)

# 3.5 Tune
xgb_tuned <- tune_grid(
  xgb_wf,
  resamples = cross_validation_folds,
  grid = xgb_grid,
  metrics = metric_set(roc_auc, accuracy)
)

# 3.6 Select best
xgb_best <- select_best(xgb_tuned, metric = "roc_auc")

# 3.7 Finalize
final_xgb_wf <- finalize_workflow(xgb_wf, xgb_best)


# Last fit
final_xgb_fit <- last_fit(
  final_xgb_wf,
  split = data_split,
  metrics = metric_set(roc_auc, accuracy)
)

# Collect metrics
collect_metrics(final_xgb_fit)
```

